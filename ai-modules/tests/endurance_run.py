# -*- coding: utf-8 -*-
"""Untitled8.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xSdzogDrAQ1lQMPcxG-szAEpmvLjv-Bd
"""



"""# Task
Analyze a running video for endurance metrics and potential cheating, including pose estimation, distance/speed calculation, fatigue detection, and visualization of results. Optionally, use GPS data for more accurate distance and speed.

## Environment setup

### Subtask:
Install necessary libraries (mediapipe, opencv-python-headless, etc.). Detect GPU availability and print environment information.

**Reasoning**:
The first step is to install the necessary libraries using pip.
"""

# Commented out IPython magic to ensure Python compatibility.
# %pip install mediapipe opencv-python-headless numpy pandas scipy matplotlib geopy

"""**Reasoning**:
Now that the libraries are installed, I will check for GPU availability and print the versions of the key installed libraries and the Python version as requested in the instructions.


"""

import tensorflow as tf
import mediapipe as mp
import cv2
import numpy as np
import pandas as pd
import scipy
import matplotlib
import platform

print("Checking for GPU availability...")
gpu_available = tf.config.list_physical_devices('GPU')
if gpu_available:
    print(f"GPU detected and available: {gpu_available}")
else:
    print("No GPU detected or available.")

print("\nEnvironment Information:")
print(f"Python Version: {platform.python_version()}")
print(f"MediaPipe Version: {mp.__version__}")
print(f"OpenCV (headless) Version: {cv2.__version__}")
print(f"NumPy Version: {np.__version__}")
print(f"Pandas Version: {pd.__version__}")
print(f"SciPy Version: {scipy.__version__}")
print(f"Matplotlib Version: {matplotlib.__version__}")

"""## Input handling

### Subtask:
Provide code to upload video and optional GPS data. Extract and print video properties (FPS, frames, resolution).

**Reasoning**:
Provide code to allow the user to upload video and optional GPS data, then use OpenCV to open the uploaded video file and extract its properties.
"""

from google.colab import files

# Upload video file
print("Please upload the video file (e.g., .mp4):")
video_upload = files.upload()

# Assuming only one file is uploaded, get the filename
if video_upload:
    video_filename = list(video_upload.keys())[0]
    print(f"Video file '{video_filename}' uploaded successfully.")
else:
    video_filename = None
    print("No video file uploaded.")

# Optionally upload GPS data file
print("\nPlease upload the optional GPS data file (e.g., .csv, .gpx), or skip this step:")
gps_upload = files.upload()

# Assuming only one file is uploaded, get the filename
if gps_upload:
    gps_filename = list(gps_upload.keys())[0]
    print(f"GPS file '{gps_filename}' uploaded successfully.")
else:
    gps_filename = None
    print("No GPS file uploaded.")

# Open the video file using OpenCV
if video_filename:
    cap = cv2.VideoCapture(video_filename)

    if not cap.isOpened():
        print(f"Error: Could not open video file {video_filename}")
        cap = None
    else:
        # Extract video properties
        fps = cap.get(cv2.CAP_PROP_FPS)
        frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
        frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))

        # Print video properties
        print("\nVideo Properties:")
        print(f"FPS: {fps}")
        print(f"Total Frames: {frame_count}")
        print(f"Resolution: {frame_width}x{frame_height}")

        # Store the video capture object (cap) for later use
        # The 'cap' variable is already storing the object.
else:
    cap = None
    print("Cannot process video properties without a video file.")

"""## Pose estimation (mediapipe blazepose)

### Subtask:
Implement pose detection to get landmark data (shoulders, hips, knees, ankles, feet). Compute per-frame metrics like stride length, step rate, and torso tilt.

**Reasoning**:
Implement pose detection using MediaPipe, extract relevant landmarks, and calculate per-frame metrics like stride length, step rate, and torso tilt, storing all this data in a pandas DataFrame.
"""

# Initialize MediaPipe Pose model
mp_pose = mp.solutions.pose
pose = mp_pose.Pose(static_image_mode=False, min_detection_confidence=0.5, min_tracking_confidence=0.5)

# List to store per-frame data
frame_data = []

# Reset the video capture to the beginning
if cap is not None:
    cap.set(cv2.CAP_PROP_POS_FRAMES, 0)

    # Loop through each frame
    while cap.isOpened():
        ret, frame = cap.read()
        if not ret:
            break

        # Convert the frame to RGB
        image_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)

        # Process the frame with MediaPipe Pose
        results = pose.process(image_rgb)

        # Store landmark data and calculated metrics
        frame_info = {'frame_index': int(cap.get(cv2.CAP_PROP_POS_FRAMES)) - 1}

        if results.pose_landmarks:
            landmarks = results.pose_landmarks.landmark

            # Extract relevant landmark coordinates
            landmark_coords = {}
            relevant_landmarks = {
                'left_shoulder': mp_pose.PoseLandmark.LEFT_SHOULDER,
                'right_shoulder': mp_pose.PoseLandmark.RIGHT_SHOULDER,
                'left_hip': mp_pose.PoseLandmark.LEFT_HIP,
                'right_hip': mp_pose.PoseLandmark.RIGHT_HIP,
                'left_knee': mp_pose.PoseLandmark.LEFT_KNEE,
                'right_knee': mp_pose.PoseLandmark.RIGHT_KNEE,
                'left_ankle': mp_pose.PoseLandmark.LEFT_ANKLE,
                'right_ankle': mp_pose.PoseLandmark.RIGHT_ANKLE,
                'left_foot_index': mp_pose.PoseLandmark.LEFT_FOOT_INDEX,
                'right_foot_index': mp_pose.PoseLandmark.RIGHT_FOOT_INDEX
            }

            for name, landmark_enum in relevant_landmarks.items():
                lm = landmarks[landmark_enum]
                landmark_coords[f'{name}_x'] = lm.x
                landmark_coords[f'{name}_y'] = lm.y
                landmark_coords[f'{name}_z'] = lm.z # Include z for potential 3D analysis later

            frame_info.update(landmark_coords)

            # Calculate per-frame metrics
            # Stride Length (example: distance between ankle landmarks)
            if 'left_ankle_x' in landmark_coords and 'right_ankle_x' in landmark_coords:
                 # Using Euclidean distance, assuming a flat ground plane for simplicity
                 stride_length = np.sqrt((landmark_coords['left_ankle_x'] - landmark_coords['right_ankle_x'])**2 +
                                         (landmark_coords['left_ankle_y'] - landmark_coords['right_ankle_y'])**2)
                 frame_info['stride_length'] = stride_length
            else:
                 frame_info['stride_length'] = np.nan


            # Torso Tilt (example: angle between vector from right hip to right shoulder and vertical axis)
            if 'right_hip_x' in landmark_coords and 'right_hip_y' in landmark_coords and \
               'right_shoulder_x' in landmark_coords and 'right_shoulder_y' in landmark_coords:
                # Vector from hip to shoulder
                hip_to_shoulder = np.array([landmark_coords['right_shoulder_x'] - landmark_coords['right_hip_x'],
                                            landmark_coords['right_shoulder_y'] - landmark_coords['right_hip_y']])
                # Vertical axis vector (pointing downwards)
                vertical_axis = np.array([0, 1])

                # Calculate dot product and magnitudes
                dot_product = np.dot(hip_to_shoulder, vertical_axis)
                magnitude_hip_shoulder = np.linalg.norm(hip_to_shoulder)
                magnitude_vertical = np.linalg.norm(vertical_axis)

                if magnitude_hip_shoulder != 0 and magnitude_vertical != 0:
                    # Calculate cosine of the angle
                    cosine_angle = dot_product / (magnitude_hip_shoulder * magnitude_vertical)
                    # Ensure cosine is within valid range [-1, 1] due to potential floating point inaccuracies
                    cosine_angle = np.clip(cosine_angle, -1.0, 1.0)
                    # Calculate angle in radians and convert to degrees
                    angle_rad = np.arccos(cosine_angle)
                    torso_tilt_deg = np.degrees(angle_rad)
                    frame_info['torso_tilt_deg'] = torso_tilt_deg
                else:
                     frame_info['torso_tilt_deg'] = np.nan
            else:
                 frame_info['torso_tilt_deg'] = np.nan

            # Step Rate (requires analysis across multiple frames, will be calculated later)
            # For now, initialize with NaN
            frame_info['step_rate'] = np.nan


        else:
             # No pose detected in this frame, fill with NaN
             frame_info.update({f'{name}_{coord}': np.nan for name in relevant_landmarks for coord in ['x', 'y', 'z']})
             frame_info['stride_length'] = np.nan
             frame_info['torso_tilt_deg'] = np.nan
             frame_info['step_rate'] = np.nan


        frame_data.append(frame_info)


    # Release the video capture object
    cap.release()

    # Create a pandas DataFrame from the collected data
    pose_df = pd.DataFrame(frame_data)

    # Display the first few rows of the DataFrame
    display(pose_df.head())

else:
    print("Video capture object is not initialized.")
    pose_df = pd.DataFrame() # Create an empty DataFrame if no video

"""**Reasoning**:
The previous code failed because the `relevant_landmarks` dictionary was not defined when trying to update `frame_info` in the `else` block (when no pose was detected). The code needs to define `relevant_landmarks` before this block. I will regenerate the code with the fix.


"""

# Initialize MediaPipe Pose model
mp_pose = mp.solutions.pose
pose = mp_pose.Pose(static_image_mode=False, min_detection_confidence=0.5, min_tracking_confidence=0.5)

# List to store per-frame data
frame_data = []

# Reset the video capture to the beginning
if cap is not None:
    cap.set(cv2.CAP_PROP_POS_FRAMES, 0)

    # Define relevant landmarks
    relevant_landmarks = {
        'left_shoulder': mp_pose.PoseLandmark.LEFT_SHOULDER,
        'right_shoulder': mp_pose.PoseLandmark.RIGHT_SHOULDER,
        'left_hip': mp_pose.PoseLandmark.LEFT_HIP,
        'right_hip': mp_pose.PoseLandmark.RIGHT_HIP,
        'left_knee': mp_pose.PoseLandmark.LEFT_KNEE,
        'right_knee': mp_pose.PoseLandmark.RIGHT_KNEE,
        'left_ankle': mp_pose.PoseLandmark.LEFT_ANKLE,
        'right_ankle': mp_pose.PoseLandmark.RIGHT_ANKLE,
        'left_foot_index': mp_pose.PoseLandmark.LEFT_FOOT_INDEX,
        'right_foot_index': mp_pose.PoseLandmark.RIGHT_FOOT_INDEX
    }

    # Loop through each frame
    while cap.isOpened():
        ret, frame = cap.read()
        if not ret:
            break

        # Convert the frame to RGB
        image_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)

        # Process the frame with MediaPipe Pose
        results = pose.process(image_rgb)

        # Store landmark data and calculated metrics
        frame_info = {'frame_index': int(cap.get(cv2.CAP_PROP_POS_FRAMES)) - 1}

        if results.pose_landmarks:
            landmarks = results.pose_landmarks.landmark

            # Extract relevant landmark coordinates
            landmark_coords = {}

            for name, landmark_enum in relevant_landmarks.items():
                lm = landmarks[landmark_enum]
                landmark_coords[f'{name}_x'] = lm.x
                landmark_coords[f'{name}_y'] = lm.y
                landmark_coords[f'{name}_z'] = lm.z # Include z for potential 3D analysis later

            frame_info.update(landmark_coords)

            # Calculate per-frame metrics
            # Stride Length (example: distance between ankle landmarks)
            if 'left_ankle_x' in landmark_coords and 'right_ankle_x' in landmark_coords:
                 # Using Euclidean distance, assuming a flat ground plane for simplicity
                 stride_length = np.sqrt((landmark_coords['left_ankle_x'] - landmark_coords['right_ankle_x'])**2 +
                                         (landmark_coords['left_ankle_y'] - landmark_coords['right_ankle_y'])**2)
                 frame_info['stride_length'] = stride_length
            else:
                 frame_info['stride_length'] = np.nan


            # Torso Tilt (example: angle between vector from right hip to right shoulder and vertical axis)
            if 'right_hip_x' in landmark_coords and 'right_hip_y' in landmark_coords and \
               'right_shoulder_x' in landmark_coords and 'right_shoulder_y' in landmark_coords:
                # Vector from hip to shoulder
                hip_to_shoulder = np.array([landmark_coords['right_shoulder_x'] - landmark_coords['right_hip_x'],
                                            landmark_coords['right_shoulder_y'] - landmark_coords['right_hip_y']])
                # Vertical axis vector (pointing downwards)
                vertical_axis = np.array([0, 1])

                # Calculate dot product and magnitudes
                dot_product = np.dot(hip_to_shoulder, vertical_axis)
                magnitude_hip_shoulder = np.linalg.norm(hip_to_shoulder)
                magnitude_vertical = np.linalg.norm(vertical_axis)

                if magnitude_hip_shoulder != 0 and magnitude_vertical != 0:
                    # Calculate cosine of the angle
                    cosine_angle = dot_product / (magnitude_hip_shoulder * magnitude_vertical)
                    # Ensure cosine is within valid range [-1, 1] due to potential floating point inaccuracies
                    cosine_angle = np.clip(cosine_angle, -1.0, 1.0)
                    # Calculate angle in radians and convert to degrees
                    angle_rad = np.arccos(cosine_angle)
                    torso_tilt_deg = np.degrees(angle_rad)
                    frame_info['torso_tilt_deg'] = torso_tilt_deg
                else:
                     frame_info['torso_tilt_deg'] = np.nan
            else:
                 frame_info['torso_tilt_deg'] = np.nan

            # Step Rate (requires analysis across multiple frames, will be calculated later)
            # For now, initialize with NaN
            frame_info['step_rate'] = np.nan


        else:
             # No pose detected in this frame, fill with NaN
             frame_info.update({f'{name}_{coord}': np.nan for name in relevant_landmarks for coord in ['x', 'y', 'z']})
             frame_info['stride_length'] = np.nan
             frame_info['torso_tilt_deg'] = np.nan
             frame_info['step_rate'] = np.nan


        frame_data.append(frame_info)


    # Release the video capture object
    cap.release()

    # Create a pandas DataFrame from the collected data
    pose_df = pd.DataFrame(frame_data)

    # Display the first few rows of the DataFrame
    display(pose_df.head())

else:
    print("Video capture object is not initialized.")
    pose_df = pd.DataFrame() # Create an empty DataFrame if no video

"""## Distance & speed estimation

### Subtask:
Calculate distance using GPS data (haversine) or step count and average stride (video only). Compute instantaneous speed, average speed, and cumulative distance.

**Reasoning**:
Implement the logic to calculate distance and speed based on either GPS data (if available) or video analysis (if GPS data is not available), and store these in the pose_df DataFrame.
"""

from geopy.distance import geodesic

# Check if GPS data is available (based on whether gps_upload is not empty)
if gps_upload:
    # Assuming GPS data is in a CSV file
    try:
        gps_filename = list(gps_upload.keys())[0]
        gps_df = pd.read_csv(gps_filename)

        # Ensure required columns exist
        if all(col in gps_df.columns for col in ['latitude', 'longitude', 'timestamp']):
            # Sort GPS data by timestamp
            gps_df['timestamp'] = pd.to_datetime(gps_df['timestamp'])
            gps_df = gps_df.sort_values('timestamp').reset_index(drop=True)

            # Calculate distance between consecutive points (Haversine formula)
            distances_meters = [0]
            for i in range(1, len(gps_df)):
                point1 = (gps_df.loc[i-1, 'latitude'], gps_df.loc[i-1, 'longitude'])
                point2 = (gps_df.loc[i, 'latitude'], gps_df.loc[i, 'longitude'])
                distance = geodesic(point1, point2).meters
                distances_meters.append(distance)

            gps_df['segment_distance_meters'] = distances_meters

            # Calculate time difference between consecutive points
            time_diffs_sec = gps_df['timestamp'].diff().dt.total_seconds().fillna(0)
            gps_df['segment_time_diff_sec'] = time_diffs_sec

            # Compute instantaneous speed (m/s)
            # Avoid division by zero for time differences of 0
            gps_df['instantaneous_speed_mps'] = gps_df.apply(
                lambda row: row['segment_distance_meters'] / row['segment_time_diff_sec'] if row['segment_time_diff_sec'] > 0 else 0, axis=1
            )

            # Compute cumulative distance
            gps_df['cumulative_distance_meters'] = gps_df['segment_distance_meters'].cumsum()

            # --- Interpolate/Align GPS data with Video Frames ---
            # This is a simplification. A proper alignment would match timestamps.
            # For this task, we'll assume a linear relationship or simple mapping.
            # A more robust solution would involve finding the closest GPS timestamp
            # for each video frame timestamp (if video timestamps were available)
            # or interpolating GPS data to match video frame times.

            # Simple approach: Assume video frames are evenly spaced between the first and last GPS points.
            # This requires knowing the video start/end time relative to GPS or interpolating GPS timestamps
            # based on video frame index and FPS.

            # Let's interpolate GPS cumulative distance and speed to match video frames
            # Create a time series for video frames based on FPS
            video_start_time = gps_df['timestamp'].iloc[0] # Assume video starts with the first GPS point
            frame_times = [video_start_time + pd.Timedelta(seconds=i/fps) for i in range(len(pose_df))]

            # Interpolate cumulative distance and instantaneous speed onto video frame times
            # Use pandas interpolation on a time series index
            gps_ts = gps_df.set_index('timestamp')
            frame_ts = pd.Series(index=frame_times).sort_index() # Create a series with video frame times as index

            # Combine GPS and frame time indices for interpolation
            combined_ts = gps_ts[['cumulative_distance_meters', 'instantaneous_speed_mps']].reindex(gps_ts.index.union(frame_ts.index)).sort_index()

            # Interpolate missing values (which are the video frame times)
            interpolated_data = combined_ts.interpolate(method='time')

            # Extract interpolated data for video frame times
            pose_df['cumulative_distance_meters'] = interpolated_data.loc[frame_times, 'cumulative_distance_meters'].values
            pose_df['instantaneous_speed_mps'] = interpolated_data.loc[frame_times, 'instantaneous_speed_mps'].values


            # Calculate overall average speed from GPS data
            total_distance_gps = gps_df['cumulative_distance_meters'].iloc[-1]
            total_time_gps = (gps_df['timestamp'].iloc[-1] - gps_df['timestamp'].iloc[0]).total_seconds()
            average_speed_mps = total_distance_gps / total_time_gps if total_time_gps > 0 else 0

            # Add average speed (constant for all frames) to pose_df
            pose_df['average_speed_mps'] = average_speed_mps

            print("Distance and speed calculated using GPS data.")
            display(pose_df[['frame_index', 'cumulative_distance_meters', 'instantaneous_speed_mps', 'average_speed_mps']].head())


        else:
            print("GPS data file does not contain required columns ('latitude', 'longitude', 'timestamp').")
            # Proceed with video-only calculation
            gps_upload = {} # Reset gps_upload to trigger video-only path
    except Exception as e:
        print(f"Error processing GPS data: {e}")
        # Proceed with video-only calculation
        gps_upload = {} # Reset gps_upload to trigger video-only path


# If GPS data was not uploaded or processing failed, use video data
if not gps_upload:
    print("No valid GPS data found. Calculating distance and speed using video data.")

    # Calculate average stride length from available data
    # Handle potential NaN values by taking the mean of non-NaN values
    average_stride_length_pixels = pose_df['stride_length'].mean()

    if pd.isna(average_stride_length_pixels) or average_stride_length_pixels == 0:
        print("Warning: Could not calculate average stride length from video data.")
        # If stride length cannot be calculated, we cannot estimate distance/speed from video.
        # Fill distance/speed columns with NaN or 0.
        pose_df['estimated_distance_pixels'] = 0
        pose_df['instantaneous_speed_pps'] = 0
        pose_df['cumulative_distance_pixels'] = 0
        pose_df['average_speed_pps'] = 0 # Average speed will also be 0
        print("Distance and speed columns filled with zeros due to lack of stride data.")

    else:
        # Estimate distance for each frame (simplification: assume one 'step' per frame
        # where stride length is detected, or use average stride length if step detection
        # is more complex). A more accurate method would involve step detection first.
        # For simplicity, we'll use the average stride length as a proxy for distance moved per frame
        # when a stride is likely occurring (indicated by a non-NaN stride_length).
        # This is a very rough estimation without actual step counting.

        # Let's refine: assume movement happens in frames where stride_length is NOT NaN
        # and the distance moved is proportional to the detected stride length.
        # A more robust approach would detect individual steps and sum stride lengths.
        # For this simplified task, we'll just use the average stride length for frames
        # where a stride was detected.

        # Create a column for estimated distance per frame in pixels
        # Use the calculated stride_length if available, otherwise use a proxy or 0.
        # Simplification: Use average stride length for frames where any pose was detected.
        # This is still not ideal, but better than using it for ALL frames.
        # A proper approach needs step detection.
        # Let's use the calculated 'stride_length' for frames where it's not NaN,
        # and 0 for frames where it is NaN. This implicitly assumes stride_length detection
        # indicates movement.

        pose_df['estimated_distance_pixels'] = pose_df['stride_length'].fillna(0)

        # Compute instantaneous speed (pixels per second)
        # Speed = Distance per frame * FPS
        pose_df['instantaneous_speed_pps'] = pose_df['estimated_distance_pixels'] * fps

        # Compute cumulative distance (pixels)
        pose_df['cumulative_distance_pixels'] = pose_df['estimated_distance_pixels'].cumsum()

        # Calculate overall average speed (pixels per second)
        total_distance_pixels = pose_df['cumulative_distance_pixels'].iloc[-1]
        total_time_sec = frame_count / fps if fps > 0 else 0 # Total video duration
        average_speed_pps = total_distance_pixels / total_time_sec if total_time_sec > 0 else 0

        # Add average speed (constant for all frames) to pose_df
        pose_df['average_speed_pps'] = average_speed_pps


        print("Distance and speed calculated using video data (pixels).")
        display(pose_df[['frame_index', 'cumulative_distance_pixels', 'instantaneous_speed_pps', 'average_speed_pps']].head())

    # Rename columns for consistency if needed, or add unit suffix
    # Sticking to unit suffixes (meters for GPS, pixels for video) is clearer.


# Ensure required columns exist after either path, filling with NaN if they don't
# This handles cases where neither GPS nor video data yielded results
distance_col = 'cumulative_distance_meters' if gps_upload else 'cumulative_distance_pixels'
instant_speed_col = 'instantaneous_speed_mps' if gps_upload else 'instantaneous_speed_pps'
average_speed_col = 'average_speed_mps' if gps_upload else 'average_speed_pps'


if distance_col not in pose_df.columns:
    pose_df[distance_col] = np.nan
if instant_speed_col not in pose_df.columns:
    pose_df[instant_speed_col] = np.nan
if average_speed_col not in pose_df.columns:
    pose_df[average_speed_col] = np.nan

"""## Endurance metrics

### Subtask:
Calculate endurance metrics like cadence, estimated VO2 Max, consistency index, and detect fatigue based on stride reduction.

**Reasoning**:
Implement the calculations for cadence, estimated VO2 Max, consistency index, and fatigue detection based on the instructions, adding the results as new columns to the pose_df DataFrame.
"""

# 1. Calculate Cadence (Steps per minute)
# This is a simplified step detection by identifying significant changes in vertical ankle position.
# A more robust approach would involve analyzing the trajectory of multiple foot/leg landmarks.
# Let's look for peaks in the vertical position of one of the ankles (e.g., left ankle)
# We'll use a simple peak detection method.

# Use the right ankle as an example
ankle_y = pose_df['right_ankle_y'].dropna()
frame_indices = ankle_y.index

# Find peaks (local minima in y, since higher y means lower on the screen)
# We are looking for when the ankle is at its lowest point during a stride/step.
# Use scipy.signal.find_peaks, looking for 'prominence' to avoid minor fluctuations.
# The distance parameter can help separate consecutive steps.
# Let's estimate a reasonable distance based on FPS and expected step rate (e.g., 150-180 steps/min -> 2.5-3 steps/sec)
# At 24 FPS, this is roughly a peak every 8-10 frames. Let's use a distance of 5-8 frames as a starting point.
from scipy.signal import find_peaks

peaks, _ = find_peaks(-ankle_y, prominence=0.01, distance=8) # Find peaks in inverted ankle_y (low y is high peak)

# The number of steps is roughly the number of detected peaks.
# Since we are detecting peaks for one ankle, this roughly corresponds to the number of steps taken by that leg.
# Total steps could be approximately twice this number, or we can consider each peak a 'step event'.
# Let's count the number of peak *events* for one foot as the number of steps.
# A more accurate cadence might involve analyzing both feet.
# For simplicity, let's consider each detected peak in one ankle's vertical position as one step.

num_steps = len(peaks)
total_time_sec = frame_count / fps if fps > 0 else 0
total_time_minutes = total_time_sec / 60 if total_time_sec > 0 else 0

cadence = (num_steps / total_time_minutes) if total_time_minutes > 0 else 0

# Add cadence as a column (constant for all frames in this simplified approach)
pose_df['cadence'] = cadence


# 2. Estimate VO2 Max
# This is highly simplified without heart rate or precise speed calibration.
# A common field test approximation (like the Cooper test or 1-mile walk test) is based on distance covered and time.
# Since we have estimated speed/distance from video, we can use a very rough approximation.
# The Daniels' Running Formula uses VDOT (equivalent to VO2 max) based on race pace.
# VDOT = -4.60 + 0.182268 * speed_mps + 0.000104 * speed_mps^2
# where speed is in meters per minute.
# We have speed in pixels per second or meters per second (if GPS was used).
# Let's use the average speed calculated earlier.

# Determine which speed column to use
average_speed_col = 'average_speed_mps' if gps_upload else 'average_speed_pps'
average_speed = pose_df[average_speed_col].iloc[0] # Average speed is constant

# Convert average speed to appropriate units for a formula.
# If using pixels/sec, this conversion is highly inaccurate without pixel-to-meter calibration.
# Let's assume, for the sake of demonstrating the calculation, that 100 pixels/sec roughly equals 1 m/s.
# This is a placeholder and needs proper calibration for real-world use.
if average_speed_col == 'average_speed_pps':
    # Placeholder conversion: 100 pixels/sec ~ 1 m/s
    average_speed_mps_estimated = average_speed / 100.0
    print(f"Using estimated speed for VO2 Max calculation (assuming 100 pixels/sec = 1 m/s): {average_speed_mps_estimated:.2f} m/s")
else: # average_speed_mps
    average_speed_mps_estimated = average_speed
    print(f"Using GPS speed for VO2 Max calculation: {average_speed_mps_estimated:.2f} m/s")


# Convert m/s to m/min
average_speed_mpm = average_speed_mps_estimated * 60

# Apply a simplified VDOT/VO2 Max formula (e.g., based on speed)
# Formula from https://runsmartproject.com/calculator/vdottable/
# VDOT = -4.60 + 0.182268 * speed_mpm + 0.000104 * speed_mpm^2
# This formula is based on race performances, using average speed from a video is a very rough estimate.
estimated_vo2_max = -4.60 + 0.182268 * average_speed_mpm + 0.000104 * (average_speed_mpm**2)

# VO2 Max should be positive, clamp at a minimum reasonable value if calculation yields negative
estimated_vo2_max = max(estimated_vo2_max, 10) # VO2 max is typically >= ~10

# Add estimated VO2 Max as a column (constant for all frames)
pose_df['estimated_vo2_max'] = estimated_vo2_max


# 3. Calculate Consistency Index
# Use the standard deviation of stride length as a measure of consistency.
# Lower standard deviation indicates higher consistency.
# Calculate this over the frames where stride length was detected.

consistency_index = pose_df['stride_length'].std()

# Add consistency index as a column (constant for all frames)
pose_df['consistency_index'] = consistency_index


# 4. Detect Fatigue based on Stride Reduction
# Analyze the trend in stride length over time.
# Look for a significant decrease or increased variability towards the end of the video.
# A simple approach: Compare average stride length in the last X% of frames to the first X% of frames.
# Or, use a rolling average and look for a downward trend.
# Let's use a rolling average and check for a significant drop.

window_size = max(10, int(frame_count * 0.05)) # Use a window of at least 10 frames or 5% of video length
rolling_stride_length = pose_df['stride_length'].rolling(window=window_size, center=True).mean()

# Calculate the percentage change from the start of the rolling average to the end.
# Use the first and last non-NaN values of the rolling average.
first_avg_stride = rolling_stride_length.dropna().iloc[0] if not rolling_stride_length.dropna().empty else np.nan
last_avg_stride = rolling_stride_length.dropna().iloc[-1] if not rolling_stride_length.dropna().empty else np.nan

# Define a threshold for significant reduction (e.g., 10% drop)
fatigue_threshold_percent = 10

fatigue_detected = False
fatigue_score = 0 # A score from 0 to 1, indicating degree of fatigue

if not pd.isna(first_avg_stride) and not pd.isna(last_avg_stride) and first_avg_stride > 0:
    stride_reduction_percent = ((first_avg_stride - last_avg_stride) / first_avg_stride) * 100

    if stride_reduction_percent > fatigue_threshold_percent:
        fatigue_detected = True
        # Simple fatigue score: proportional to how much the reduction exceeds the threshold
        fatigue_score = min(1.0, (stride_reduction_percent - fatigue_threshold_percent) / (100 - fatigue_threshold_percent)) # Max score 1.0

# Add fatigue detection results
pose_df['fatigue_detected'] = fatigue_detected
pose_df['fatigue_score'] = fatigue_score


# Display the relevant columns to verify
display(pose_df[['frame_index', 'cadence', 'estimated_vo2_max', 'consistency_index', 'fatigue_detected', 'fatigue_score']].head())
display(pose_df[['frame_index', 'cadence', 'estimated_vo2_max', 'consistency_index', 'fatigue_detected', 'fatigue_score']].tail())

"""## Cheat / manipulation detection

### Subtask:
Implement various checks for inconsistencies in frames, pose motion, background motion, GPS data, landmark confidence, and audio (optional). Log cheating events and calculate an overall cheating probability. Overlay warnings on the video.

**Reasoning**:
Implement checks for various inconsistencies including frame drops, pose motion, background changes, GPS data (if available), and landmark confidence. Store detected cheating events and update the DataFrame with anomaly scores or flags.
"""

# 1. Initialize a dictionary/list to store detected cheating events
cheating_events = []

# Add columns for cheating detection results to pose_df
pose_df['frame_anomaly_score'] = 0.0 # Score for anomalies in this frame
pose_df['cheating_event_type'] = None # Type of cheating event detected in this frame (e.g., 'frame_drop', 'pose_jump')

# 2. Check for frame inconsistencies (frame drops)
# The frame_index in pose_df should be sequential (0, 1, 2, ...)
# Check for gaps in frame_index
expected_frame_indices = pd.Series(range(len(pose_df)))
missing_frames = expected_frame_indices[~expected_frame_indices.isin(pose_df['frame_index'])]

if not missing_frames.empty:
    for frame_idx in missing_frames:
        event = {
            'frame_index': frame_idx,
            'type': 'frame_drop',
            'description': f'Missing frame {frame_idx}'
        }
        cheating_events.append(event)
        # We cannot add a score to the missing frame in pose_df,
        # but we can potentially flag the *surrounding* frames or aggregate later.
        # For now, just log the event.
    print(f"Detected {len(missing_frames)} missing frames.")
else:
    print("No missing frames detected.")

# 3. Check for sudden, unrealistic jumps in landmark positions (pose motion)
# Calculate the change in position for a few key landmarks between consecutive frames.
# Use Euclidean distance in normalized coordinates.

# Define a threshold for a significant jump (e.g., a large percentage of frame dimensions)
# This threshold might need tuning based on video resolution and movement scale.
# Let's use a value that represents a significant portion of the frame height/width, e.g., 10% of the smaller dimension.
jump_threshold = min(frame_width, frame_height) * 0.05 # Example: 5% of the smaller dimension in pixels, converted to normalized coordinates

# Columns to check for jumps
landmark_cols = [col for col in pose_df.columns if '_x' in col or '_y' in col]

for i in range(1, len(pose_df)):
    frame_index = pose_df.loc[i, 'frame_index']
    prev_frame_index = pose_df.loc[i-1, 'frame_index']

    # Only check if consecutive frames are present (no frame drop between them)
    if frame_index == prev_frame_index + 1:
        jump_detected = False
        for landmark_name in relevant_landmarks.keys(): # Use the relevant_landmarks defined earlier
            x_col, y_col = f'{landmark_name}_x', f'{landmark_name}_y'
            if x_col in pose_df.columns and y_col in pose_df.columns:
                p1_x, p1_y = pose_df.loc[i-1, x_col], pose_df.loc[i-1, y_col]
                p2_x, p2_y = pose_df.loc[i, x_col], pose_df.loc[i, y_col]

                # Check if both points are valid (not NaN)
                if pd.notna(p1_x) and pd.notna(p1_y) and pd.notna(p2_x) and pd.notna(p2_y):
                    # Calculate Euclidean distance in normalized coordinates
                    distance = np.sqrt((p2_x - p1_x)**2 + (p2_y - p1_y)**2)

                    # Compare distance to the threshold (normalized)
                    # Need to normalize the jump_threshold calculated based on pixels
                    # Assuming normalized coordinates are relative to frame dimensions (0-1)
                    # A jump of 5% of frame height in normalized coords is 0.05
                    normalized_jump_threshold = 0.05 # This threshold is in normalized units (0 to 1)

                    if distance > normalized_jump_threshold:
                        jump_detected = True
                        event = {
                            'frame_index': frame_index,
                            'type': 'pose_jump',
                            'description': f'Sudden jump in {landmark_name} position ({distance:.4f})',
                            'details': {'landmark': landmark_name, 'jump_distance': distance}
                        }
                        cheating_events.append(event)
                        # Increment anomaly score for this frame
                        pose_df.loc[i, 'frame_anomaly_score'] += 1 # Simple count of detected jumps
                        # Break here if we only need to flag the frame once for a jump
                        # or continue to log all jumps in the frame. Let's log all for now.

        if jump_detected:
             pose_df.loc[i, 'cheating_event_type'] = 'pose_jump' # Assign type if any jump occurred


print(f"Detected {len([e for e in cheating_events if e['type'] == 'pose_jump'])} potential pose jumps.")


# 4. Check for significant changes in overall frame content (background motion)
# This is complex and resource-intensive. A basic approach: Compare mean pixel intensity or simple image hash.
# A more robust method would use structural similarity index (SSIM) or histogram comparison.
# Let's use a simple mean pixel intensity change as a placeholder.
# This requires re-reading frames or having them in memory, which might be slow for large videos.
# Given the current setup, re-reading frames is the most straightforward way.

# Reset video capture to the beginning
if cap is not None:
    cap.set(cv2.CAP_PROP_POS_FRAMES, 0)
    ret, prev_frame = cap.read()
    if ret:
        prev_mean_intensity = np.mean(prev_frame)

        for i in range(1, len(pose_df)):
            ret, current_frame = cap.read()
            if not ret:
                break

            current_mean_intensity = np.mean(current_frame)

            # Calculate absolute difference in mean intensity
            intensity_diff = abs(current_mean_intensity - prev_mean_intensity)

            # Define a threshold for significant change (needs tuning)
            # This threshold depends heavily on video content and encoding.
            # A simple percentage of max intensity (255 for 8-bit)
            intensity_threshold = 255 * 0.1 # Example: 10% change in mean intensity

            if intensity_diff > intensity_threshold:
                 # This might indicate a scene change, cut, or background manipulation.
                 # It's a very rough indicator.
                 event = {
                     'frame_index': i,
                     'type': 'background_change',
                     'description': f'Significant change in mean frame intensity ({intensity_diff:.2f})',
                     'details': {'intensity_difference': intensity_diff}
                 }
                 cheating_events.append(event)
                 # Increment anomaly score
                 pose_df.loc[i, 'frame_anomaly_score'] += 0.5 # Add a smaller score for this rough check
                 # Assign type if not already assigned a more specific type
                 if pose_df.loc[i, 'cheating_event_type'] is None:
                     pose_df.loc[i, 'cheating_event_type'] = 'background_change'


            prev_mean_intensity = current_mean_intensity

    cap.release()
    print(f"Detected {len([e for e in cheating_events if e['type'] == 'background_change'])} potential background changes.")
else:
    print("Video capture object not available for background check.")


# 5. Check for GPS data inconsistencies (if GPS data was used)
# This check relies on the gps_df created in the previous step.
if 'gps_df' in locals() and not gps_df.empty and gps_upload:
    # Check for sudden jumps in location/speed that are physically impossible.
    # We already calculated instantaneous_speed_mps in gps_df.
    # Flag speeds exceeding a human maximum running speed (e.g., Usain Bolt ~10.4 m/s)
    max_human_speed_mps = 12 # A bit higher than world record for robustness

    for i in range(len(gps_df)):
        if gps_df.loc[i, 'instantaneous_speed_mps'] > max_human_speed_mps:
             timestamp = gps_df.loc[i, 'timestamp']
             event = {
                 'timestamp': timestamp,
                 'type': 'gps_speed_anomaly',
                 'description': f'Implausible GPS speed detected ({gps_df.loc[i, "instantaneous_speed_mps"]:.2f} m/s)',
                 'details': {'speed_mps': gps_df.loc[i, 'instantaneous_speed_mps']}
             }
             cheating_events.append(event)
             # Note: Aligning GPS anomalies to video frames in pose_df is tricky
             # without precise timestamp mapping. We'll just log the GPS event for now.
             # If needed, we could find the closest video frame and add to its score.

    print(f"Detected {len([e for e in cheating_events if e['type'] == 'gps_speed_anomaly'])} GPS speed anomalies.")
else:
    print("No GPS data or gps_df not available for inconsistency check.")


# 6. Check landmark confidence scores
# MediaPipe landmarks have a 'presence' and 'visibility' score, often combined into a confidence score.
# We need to modify the pose estimation step to store these scores.
# Since the previous step didn't store confidence, we can't check it now without re-processing.
# Assume for demonstration that we have landmark confidence scores stored.
# If confidence scores were available (e.g., in columns like 'right_ankle_confidence'):
# confidence_threshold = 0.8 # Example threshold

# for i in range(len(pose_df)):
#     low_confidence_landmarks = []
#     for landmark_name in relevant_landmarks.keys():
#         confidence_col = f'{landmark_name}_confidence' # Hypothetical column name
#         if confidence_col in pose_df.columns and pose_df.loc[i, confidence_col] < confidence_threshold:
#             low_confidence_landmarks.append(landmark_name)

#     if low_confidence_landmarks:
#         event = {
#             'frame_index': i,
#             'type': 'low_landmark_confidence',
#             'description': f'Low confidence for landmarks: {", ".join(low_confidence_landmarks)}',
#             'details': {'landmarks': low_confidence_landmarks}
#         }
#         cheating_events.append(event)
#         pose_df.loc[i, 'frame_anomaly_score'] += 0.2 # Add a smaller score
#         if pose_df.loc[i, 'cheating_event_type'] is None:
#              pose_df.loc[i, 'cheating_event_type'] = 'low_landmark_confidence'

print("Landmark confidence check skipped as confidence scores were not stored in pose_df.") # Indicate this step was skipped


# 7. Audio inconsistencies (Optional) - Skipped as per instructions and lack of audio data/processing
print("Audio inconsistency check skipped (optional and requires audio processing).")


# 8. Calculate overall cheating probability/score
# Summing the anomaly scores per frame is one way.
# Another is a weighted sum based on event types.
# A simple approach: total number of detected events / total number of frames (for frame-based events)
# Or, sum up frame_anomaly_score and normalize.

# Total frame-based anomalies
total_frame_anomalies = pose_df['frame_anomaly_score'].sum()
total_frames = len(pose_df)

# Simple cheating score based on frame anomalies
# Normalize by the maximum possible score (e.g., sum of max scores per frame)
# Max score per frame depends on how many checks contribute. Let's just use total score for now.
# A simple approach: total anomaly score divided by number of frames.
overall_cheating_score = total_frame_anomalies / total_frames if total_frames > 0 else 0

# A more interpretable score might be a percentage or a value between 0 and 1.
# Let's calculate a score based on the *presence* of different anomaly types and their severity.
# This is subjective and depends on how critical each anomaly type is considered.
# For simplicity, let's just report the total count of cheating events and the average anomaly score per frame.

total_cheating_events_count = len(cheating_events)
average_frame_anomaly_score = overall_cheating_score # This is the average score per frame


print(f"\nOverall Cheating Analysis:")
print(f"Total number of detected cheating events across all types: {total_cheating_events_count}")
# Note: total_cheating_events_count includes frame drops and GPS anomalies which might not map directly to pose_df rows.
# The frame_anomaly_score in pose_df only reflects anomalies assigned to specific frames.
print(f"Average frame anomaly score (sum of scores per frame / total frames): {average_frame_anomaly_score:.4f}")

# A cheating probability could be derived, but it's highly model-dependent.
# Let's define a simple probability based on the proportion of frames with a non-zero anomaly score.
frames_with_anomalies = pose_df[pose_df['frame_anomaly_score'] > 0].shape[0]
cheating_probability = frames_with_anomalies / total_frames if total_frames > 0 else 0
print(f"Probability of cheating based on frames with anomalies: {cheating_probability:.4f}")


# 9. Add columns to pose_df (already done in step 1 and updated in checks)
# Columns 'frame_anomaly_score' and 'cheating_event_type' are added and populated.
# 'cheating_event_type' only stores the *last* type of anomaly detected in a frame if multiple occur.
# A list column could store all types, but is less convenient for simple filtering.

# Display frames with non-zero anomaly scores
print("\nFrames with detected anomalies:")
display(pose_df[pose_df['frame_anomaly_score'] > 0][['frame_index', 'frame_anomaly_score', 'cheating_event_type']].head())
display(pose_df[pose_df['frame_anomaly_score'] > 0][['frame_index', 'frame_anomaly_score', 'cheating_event_type']].tail())


# 10. Prepare for visual warnings (already stored in pose_df)
# The 'frame_anomaly_score' and 'cheating_event_type' columns can be used in the visualization step
# to decide which frames need warnings and what type of warning to display.
# No specific output needed here, just confirmation that the data is ready in pose_df.
print("\nPose DataFrame updated with anomaly scores and event types, ready for visualization.")

"""## Cheating Analysis Report

Based on the analysis of the provided video, a cheating analysis was conducted. The following potential indicators of cheating or manipulation were checked:

*   **Frame Inconsistencies (Frame Drops):** Checks for missing frames in the video sequence.
*   **Pose Motion Sanity:** Analyzes sudden and unrealistic jumps in key pose landmark positions between consecutive frames.
*   **Background Motion Analysis:** A basic check for significant changes in overall frame content that might indicate cuts or manipulations (using mean pixel intensity difference).
*   **GPS Consistency:** (If GPS data is provided) Checks for implausible speeds or inconsistencies in GPS location data.
*   **Landmark Confidence:** (If confidence scores are available) Checks for sudden drops in the confidence scores of detected landmarks.
*   **Audio Inconsistencies:** (Optional, not implemented in this analysis) Could include checks for long silences, desync, etc.

The analysis identified potential anomalies, and a score was calculated for each frame based on the detected inconsistencies. These frame-level scores were used to derive an overall cheating probability.
"""

# Generate a formatted cheating report

print("--- Cheating Analysis Report ---")
print(f"Video File Analyzed: {video_filename if 'video_filename' in globals() else 'N/A'}")
print(f"Total Frames Analyzed: {total_frames if 'total_frames' in globals() else 'N/A'}")
print("-" * 30)

print("\nPotential Cheating Indicators:")
print(f"- Missing Frames: {len([e for e in cheating_events if e['type'] == 'frame_drop']) if 'cheating_events' in globals() else 'N/A'} detected")
print(f"- Pose Jumps: {len([e for e in cheating_events if e['type'] == 'pose_jump']) if 'cheating_events' in globals() else 'N/A'} detected")
print(f"- Background Changes (basic check): {len([e for e in cheating_events if e['type'] == 'background_change']) if 'cheating_events' in globals() else 'N/A'} detected")
print(f"- GPS Speed Anomalies: {len([e for e in cheating_events if e['type'] == 'gps_speed_anomaly']) if 'cheating_events' in globals() else 'N/A'} detected")
print("- Landmark Confidence Anomalies: Skipped (confidence scores not stored)")
print("- Audio Inconsistencies: Skipped (not implemented)")


print("-" * 30)
print("\nOverall Cheating Assessment:")

# Ensure these variables exist from the previous cheating analysis cell
overall_cheating_score_percent = overall_cheating_score * 100 if 'overall_cheating_score' in globals() else np.nan
cheating_probability_percent = cheating_probability * 100 if 'cheating_probability' in globals() else np.nan


if not np.isnan(overall_cheating_score_percent):
    print(f"Average Frame Anomaly Score: {average_frame_anomaly_score:.4f}")
    print(f"Overall Cheating Score (Average Anomaly Score %): {overall_cheating_score_percent:.2f}%")
    print(f"Probability of Cheating (based on frames with anomalies): {cheating_probability_percent:.2f}%")
else:
    print("Overall cheating metrics could not be calculated. Please ensure the cheating detection step ran successfully.")


print("\nDetected Cheating Events Details:")
if 'cheating_events' in globals() and cheating_events:
    for event in cheating_events:
        # Format output based on event type
        if event['type'] == 'frame_drop':
            print(f"- Frame Drop: Frame {event['frame_index']} is missing.")
        elif event['type'] == 'pose_jump':
             print(f"- Pose Jump: Frame {event['frame_index']} - {event['description']}")
        elif event['type'] == 'background_change':
             print(f"- Background Change: Frame {event['frame_index']} - {event['description']}")
        elif event['type'] == 'gps_speed_anomaly':
             # GPS events are logged with timestamp, need to check if timestamp exists
             timestamp_str = event.get('timestamp', 'N/A')
             print(f"- GPS Speed Anomaly: Timestamp {timestamp_str} - {event['description']}")

else:
    print("No specific cheating events were logged.")


print("-" * 30)
print("Note: This report is based on automated video analysis.")
print("Manual review of the processed video with warnings is recommended for confirmation.")
print("The probability is an estimation based on detected anomalies and thresholds.")

# Optionally, save the report to a file
report_filename = "cheating_analysis_report.txt"
with open(report_filename, "w") as f:
    f.write("--- Cheating Analysis Report ---\n")
    f.write(f"Video File Analyzed: {video_filename if 'video_filename' in globals() else 'N/A'}\n")
    f.write(f"Total Frames Analyzed: {total_frames if 'total_frames' in globals() else 'N/A'}\n")
    f.write("-" * 30 + "\n")
    f.write("\nPotential Cheating Indicators:\n")
    f.write(f"- Missing Frames: {len([e for e in cheating_events if e['type'] == 'frame_drop']) if 'cheating_events' in globals() else 'N/A'} detected\n")
    f.write(f"- Pose Jumps: {len([e for e in cheating_events if e['type'] == 'pose_jump']) if 'cheating_events' in globals() else 'N/A'} detected\n")
    f.write(f"- Background Changes (basic check): {len([e for e in cheating_events if e['type'] == 'background_change']) if 'cheating_events' in globals() else 'N/A'} detected\n")
    f.write(f"- GPS Speed Anomalies: {len([e for e in cheating_events if e['type'] == 'gps_speed_anomaly']) if 'cheating_events' in globals() else 'N/A'} detected\n")
    f.write("- Landmark Confidence Anomalies: Skipped (confidence scores not stored)\n")
    f.write("- Audio Inconsistencies: Skipped (not implemented)\n")
    f.write("-" * 30 + "\n")
    f.write("\nOverall Cheating Assessment:\n")
    if not np.isnan(overall_cheating_score_percent):
        f.write(f"Average Frame Anomaly Score: {average_frame_anomaly_score:.4f}\n")
        f.write(f"Overall Cheating Score (Average Anomaly Score %): {overall_cheating_score_percent:.2f}%\n")
        f.write(f"Probability of Cheating (based on frames with anomalies): {cheating_probability_percent:.2f}%\n")
    else:
        f.write("Overall cheating metrics could not be calculated. Please ensure the cheating detection step ran successfully.\n")
    f.write("\nDetected Cheating Events Details:\n")
    if 'cheating_events' in globals() and cheating_events:
         for event in cheating_events:
            if event['type'] == 'frame_drop':
                f.write(f"- Frame Drop: Frame {event['frame_index']} is missing.\n")
            elif event['type'] == 'pose_jump':
                 f.write(f"- Pose Jump: Frame {event['frame_index']} - {event['description']}\n")
            elif event['type'] == 'background_change':
                 f.write(f"- Background Change: Frame {event['frame_index']} - {event['description']}\n")
            elif event['type'] == 'gps_speed_anomaly':
                 timestamp_str = event.get('timestamp', 'N/A')
                 f.write(f"- GPS Speed Anomaly: Timestamp {timestamp_str} - {event['description']}\n")
    else:
        f.write("No specific cheating events were logged.\n")
    f.write("-" * 30 + "\n")
    f.write("Note: This report is based on automated video analysis.\n")
    f.write("Manual review of the processed video with warnings is recommended for confirmation.\n")
    f.write("The probability is an estimation based on detected anomalies and thresholds.\n")

print(f"\nCheating analysis report saved to '{report_filename}'")

"""## Visualization

### Subtask:
Generate a processed video with overlays for speed, distance, cadence, and warnings. Create and save plots for speed vs. time, stride vs. time, and cumulative distance.

**Reasoning**:
Initialize the video writer, reset the video capture, loop through frames, add overlays for speed, distance, cadence, and warnings, then write the frame. Finally, release the video capture and writer.
"""

# 1. Initialize a new video writer object
output_video_filename = 'processed_video.mp4'
# Define the codec
# Ensure frame_width and frame_height are defined from earlier steps (Input Handling)
if 'frame_width' not in globals() or 'frame_height' not in globals() or 'fps' not in globals():
     print("Error: Video properties (frame_width, frame_height, fps) are not defined.")
     # Set dummy values or exit if critical
     frame_width, frame_height, fps = 1280, 720, 30
     print("Using default video properties for writer initialization.")

# mp4v codec is generally compatible
fourcc = cv2.VideoWriter_fourcc(*'mp4v') # Or 'XVID'

out = cv2.VideoWriter(output_video_filename, fourcc, fps, (frame_width, frame_height))

if not out.isOpened():
    print(f"Error: Could not open video writer for {output_video_filename}")
else:
    print(f"Video writer initialized for {output_video_filename}")

# 2. Reset the original video capture object
# The 'cap' object might have been released in the pose estimation step.
# Re-open the video file if it was closed.
if cap is None or not cap.isOpened():
    if 'video_filename' in globals():
        cap = cv2.VideoCapture(video_filename)
        if not cap.isOpened():
            print(f"Error: Could not re-open video file {video_filename}")
            cap = None
        else:
            print("Video capture re-opened successfully.")
    else:
        print("Error: Original video filename not found. Cannot re-open video.")
        cap = None

if cap is not None:
    cap.set(cv2.CAP_PROP_POS_FRAMES, 0)
    print("Video capture reset to the beginning.")

    # Determine which columns to use for speed and distance based on GPS usage
    # Check if 'gps_upload' exists and is not empty
    use_gps = 'gps_upload' in globals() and bool(gps_upload)
    distance_col = 'cumulative_distance_meters' if use_gps else 'cumulative_distance_pixels'
    speed_col = 'instantaneous_speed_mps' if use_gps else 'instantaneous_speed_pps'
    distance_unit = 'm' if use_gps else 'pixels'
    speed_unit = 'm/s' if use_gps else 'pixels/s'


    # 3. Loop through each frame of the original video
    print("Processing frames and adding overlays...")
    for i in range(len(pose_df)):
        ret, frame = cap.read()
        if not ret:
            print(f"Warning: Could not read frame {i}. Stopping processing.")
            break

        # Retrieve the corresponding data row from the pose_df
        # Use .loc for label-based indexing by frame_index if needed,
        # but since pose_df is aligned by index, using iloc is simpler here.
        frame_data_row = pose_df.iloc[i]
        current_frame_index = frame_data_row['frame_index'] # Should be equal to i

        # Prepare text strings for overlays
        speed_val = frame_data_row[speed_col] if pd.notna(frame_data_row[speed_col]) else 0.0
        distance_val = frame_data_row[distance_col] if pd.notna(frame_data_row[distance_col]) else 0.0
        cadence_val = frame_data_row['cadence'] if pd.notna(frame_data_row['cadence']) else 0.0 # Cadence is constant after calculation

        # Format overlay text
        speed_text = f"Speed: {speed_val:.2f} {speed_unit}"
        distance_text = f"Distance: {distance_val:.2f} {distance_unit}"
        cadence_text = f"Cadence: {cadence_val:.2f} steps/min"

        # Add warning text if anomaly detected
        warning_text = ""
        if frame_data_row['frame_anomaly_score'] > 0:
            event_type = frame_data_row['cheating_event_type']
            warning_text = f"Warning: {event_type}" # Use the recorded event type

        # Use cv2.putText() to draw text onto the frame
        font = cv2.FONT_HERSHEY_SIMPLEX
        font_scale = 0.8
        font_thickness = 2
        text_color = (0, 0, 255) # Red color for text (BGR format)
        warning_color = (0, 0, 255) # Red for warnings

        # Text positions (top-left corner of the text)
        speed_pos = (10, 30)
        distance_pos = (10, 70)
        cadence_pos = (10, 110)
        warning_pos = (10, frame_height - 30) # Bottom-left

        cv2.putText(frame, speed_text, speed_pos, font, font_scale, text_color, font_thickness)
        cv2.putText(frame, distance_text, distance_pos, font, font_scale, text_color, font_thickness)
        cv2.putText(frame, cadence_text, cadence_pos, font, font_scale, text_color, font_thickness)

        if warning_text:
            cv2.putText(frame, warning_text, warning_pos, font, font_scale, warning_color, font_thickness)


        # Write the processed frame with overlays
        out.write(frame)

    print("Finished processing frames.")

    # 4. Release the video capture and writer objects
    cap.release()
    out.release()
    print(f"Processed video saved as '{output_video_filename}'")

else:
    print("Skipping video processing and overlay as video capture is not available.")

"""**Reasoning**:
Generate and save plots for speed vs. time, stride vs. time, and cumulative distance vs. time, then display them.


"""

import matplotlib.pyplot as plt

# 5. Create a plot for instantaneous speed vs. time (frame index)
plt.figure(figsize=(12, 6))
speed_col = 'instantaneous_speed_mps' if use_gps else 'instantaneous_speed_pps'
speed_unit = 'm/s' if use_gps else 'pixels/s'
plt.plot(pose_df['frame_index'], pose_df[speed_col])
plt.xlabel("Frame Index")
plt.ylabel(f"Speed ({speed_unit})")
plt.title("Instantaneous Speed Over Time")
plt.grid(True)
plt.savefig('instantaneous_speed_plot.png') # Save the plot

# 6. Create a plot for stride length vs. time (frame index)
plt.figure(figsize=(12, 6))
plt.plot(pose_df['frame_index'], pose_df['stride_length'])
plt.xlabel("Frame Index")
plt.ylabel("Stride Length (normalized)")
plt.title("Stride Length Over Time")
plt.grid(True)
plt.savefig('stride_length_plot.png') # Save the plot

# 7. Create a plot for cumulative distance vs. time (frame index)
plt.figure(figsize=(12, 6))
distance_col = 'cumulative_distance_meters' if use_gps else 'cumulative_distance_pixels'
distance_unit = 'm' if use_gps else 'pixels'
plt.plot(pose_df['frame_index'], pose_df[distance_col])
plt.xlabel("Frame Index")
plt.ylabel(f"Cumulative Distance ({distance_unit})")
plt.title("Cumulative Distance Over Time")
plt.grid(True)
plt.savefig('cumulative_distance_plot.png') # Save the plot

# 8. Display the generated plots
plt.show()

"""## Summary:

### Data Analysis Key Findings

*   No GPU was detected or available for processing.
*   A video file ("How to  Shuttle Run.mp4") was successfully uploaded and analyzed. No GPS data file was provided.
*   The video has a resolution of 1280x720 pixels, runs at 23.976 FPS, and contains 360 frames.
*   Pose landmarks (shoulders, hips, knees, ankles, feet) were successfully extracted for frames where a pose was detected.
*   Per-frame metrics like stride length and torso tilt were calculated from pose landmarks.
*   Due to the lack of GPS data, distance and speed were estimated using video analysis based on detected stride length. The estimated average speed was approximately 1.05 pixels/s, and the total cumulative distance was approximately 158 pixels.
*   Endurance metrics were calculated: Cadence was approximately 47.95 steps/min (based on right ankle peaks), estimated VO2 Max was calculated as 10.0 (using a placeholder conversion), and the Consistency Index (std deviation of stride length) was approximately 0.035.
*   Fatigue detection, based on a 10% reduction in rolling average stride length, did not detect fatigue in this video.
*   Several checks for potential cheating/anomalies were performed:
    *   No missing frames were detected.
    *   102 potential pose jumps (sudden changes in landmark positions) were detected.
    *   No significant background changes were detected using a simple mean intensity check.
    *   GPS speed anomalies were skipped as no GPS data was used.
    *   Landmark confidence checks were skipped as confidence scores were not stored.
*   An overall cheating analysis resulted in an average frame anomaly score of 0.2833 and a cheating probability of 0.1139 based on the proportion of frames with detected anomalies.
*   A processed video with overlays showing estimated speed, distance, cadence, and anomaly warnings was successfully generated.
*   Plots visualizing instantaneous speed, stride length, and cumulative distance over time were created and saved.

### Insights or Next Steps

*   The estimated VO2 Max and distance/speed based purely on video pixels are highly uncalibrated. Future steps should include a method to calibrate pixel measurements to real-world units if no GPS data is available (e.g., by measuring a known distance in the video scene).
*   The current step detection for cadence is simplified. Implementing a more robust step counting algorithm (e.g., analyzing vertical displacement peaks or temporal gait analysis) would improve the accuracy of cadence and potentially distance estimation from video.

"""