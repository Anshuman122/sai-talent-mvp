# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1P12bpHtFuC-078HcL_vJ9GjhWEo8zsPI
"""



"""# Task
Analyze a vertical jump video using a pre-trained model in Google Colab, saving the results to a folder.

## Setup environment

### Subtask:
Install necessary libraries and dependencies.

**Reasoning**:
Install the necessary libraries for video processing and pose estimation using pip.
"""

!pip install opencv-python mediapipe

"""## Video upload

### Subtask:
Provide code to upload the video file to the Colab environment.

**Reasoning**:
Import the necessary module and use the `files.upload()` function to upload the video file as instructed, storing the result and extracting the filename.
"""

from google.colab import files

uploaded = files.upload()

# Assuming only one file is uploaded, get the filename
video_filename = list(uploaded.keys())[0]
print(f"Uploaded file: {video_filename}")

"""## Jump Counting

### Subtask:
Count the number of jumps based on the vertical position data.

**Reasoning**:
Identify peaks in the average vertical position data to count the number of jumps. A peak in this inverted y-axis plot corresponds to the lowest vertical position, which is the apex of the jump. Use a simple peak detection algorithm.
"""

from scipy.signal import find_peaks

# Invert the y-data again for peak detection on the actual height (higher values mean higher)
inverted_y = -average_y_by_frame['y']

# Find peaks in the inverted data. Adjust the 'distance' parameter based on expected jump frequency
peaks, _ = find_peaks(inverted_y, distance=fps*0.5) # assuming at least 0.5 seconds between jumps

num_jumps = len(peaks)

print(f"Number of jumps detected: {num_jumps}")

# Optional: Plot the peaks on the inverted y-axis plot to visualize detection
plt.figure(figsize=(12, 6))
plt.plot(average_y_by_frame['frame'], inverted_y)
plt.plot(average_y_by_frame['frame'][peaks], inverted_y[peaks], "x")
plt.xlabel('Frame Number')
plt.ylabel('Inverted Vertical Position (higher is higher)')
plt.title('Inverted Average Vertical Position of Lower Body with Detected Peaks')
plt.show()

"""## Model loading

### Subtask:
Load the pre-trained model for vertical jump analysis.

**Reasoning**:
Import the necessary library for pose estimation and load the pre-trained pose estimation model.
"""

import mediapipe as mp

mp_pose = mp.solutions.pose
pose = mp_pose.Pose(static_image_mode=False, min_detection_confidence=0.5, min_tracking_confidence=0.5)

"""## Video processing and analysis

### Subtask:
Process the uploaded video frame by frame using the loaded model to detect key jump metrics (e.g., body landmarks, jump height).

**Reasoning**:
Process the uploaded video frame by frame using the loaded model to detect key jump metrics.
"""

import cv2
import numpy as np

cap = cv2.VideoCapture(video_filename)

if not cap.isOpened():
    print(f"Error: Could not open video file {video_filename}")
    # finish_task is called later, even if we fail to open the video
else:
    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
    fps = int(cap.get(cv2.CAP_PROP_FPS))

    fourcc = cv2.VideoWriter_fourcc(*'mp4v')
    out = cv2.VideoWriter('output_vertical_jump.mp4', fourcc, fps, (frame_width, frame_height))

    results_list = []
    mp_drawing = mp.solutions.drawing_utils
    mp_drawing_styles = mp.solutions.drawing_styles

    while cap.isOpened():
        ret, frame = cap.read()
        if not ret:
            break

        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        results = pose.process(frame_rgb)

        if results.pose_landmarks:
            # Store landmark data
            landmarks_data = [(landmark.x, landmark.y, landmark.z, landmark.visibility)
                              for landmark in results.pose_landmarks.landmark]
            results_list.append({'frame': int(cap.get(cv2.CAP_PROP_POS_FRAMES)), 'landmarks': landmarks_data})

            # Draw landmarks
            mp_drawing.draw_landmarks(
                frame,
                results.pose_landmarks,
                mp_pose.POSE_CONNECTIONS,
                landmark_drawing_spec=mp_drawing_styles.get_default_pose_landmarks_style())

        out.write(frame)

    cap.release()
    out.release()
    pt.show()

    print("Video processing complete. Results saved to output_vertical_jump.mp4")

"""## Results saving

### Subtask:
Save the analysis results, such as annotated videos or data, to a designated folder.

**Reasoning**:
Create a directory, move the output video into it, convert the results list to a DataFrame, and save the DataFrame as a CSV in the new directory, then print a confirmation message.
"""

import os
import pandas as pd

# 1. Create a directory
results_dir = 'vertical_jump_analysis_results'
if not os.path.exists(results_dir):
    os.makedirs(results_dir)
    print(f"Created directory: {results_dir}")

# 2. Move the generated output video file
output_video_path = 'output_vertical_jump.mp4'
if os.path.exists(output_video_path):
    new_video_path = os.path.join(results_dir, output_video_path)
    os.rename(output_video_path, new_video_path)
    print(f"Moved '{output_video_path}' to '{results_dir}'")
else:
    print(f"Warning: '{output_video_path}' not found. Skipping video move.")


# 3. Convert landmark data to DataFrame if it exists
if 'results_list' in locals() and results_list:
    # Flatten the list of dictionaries for easier DataFrame conversion
    flattened_results = []
    for item in results_list:
        frame_num = item['frame']
        for i, landmark in enumerate(item['landmarks']):
            flattened_results.append({
                'frame': frame_num,
                'landmark_index': i,
                'x': landmark[0],
                'y': landmark[1],
                'z': landmark[2],
                'visibility': landmark[3]
            })

    landmarks_df = pd.DataFrame(flattened_results)

    # 4. Save the landmark data DataFrame to a CSV file
    csv_output_path = os.path.join(results_dir, 'landmarks_data.csv')
    landmarks_df.to_csv(csv_output_path, index=False)
    print(f"Saved landmark data to '{csv_output_path}'")
else:
    print("No landmark data (results_list) found or it is empty. Skipping CSV save.")


# 5. Print confirmation message
print(f"\nAnalysis results (video and/or data) saved in the '{results_dir}' directory.")

"""## Visualization (optional)

### Subtask:
Create visualizations of the jump data, if applicable.

**Reasoning**:
Calculate the average y-position for lower body landmarks and plot it over time to visualize the vertical movement during the jump.
"""

import matplotlib.pyplot as plt

# Define indices for lower body landmarks (e.g., ankles and knees)
# Based on MediaPipe Pose documentation (example indices, adjust if needed)
# Assuming indices 27, 28 (left/right ankle) and 25, 26 (left/right knee)
lower_body_landmarks_indices = [25, 26, 27, 28]

# Filter the DataFrame to include only the lower body landmarks
lower_body_df = landmarks_df[landmarks_df['landmark_index'].isin(lower_body_landmarks_indices)]

# Calculate the average y-position for each frame
average_y_by_frame = lower_body_df.groupby('frame')['y'].mean().reset_index()

# Create the plot
plt.figure(figsize=(12, 6))
plt.plot(average_y_by_frame['frame'], average_y_by_frame['y'])

# Label the axes
plt.xlabel('Frame Number')
plt.ylabel('Vertical Position (y)')

# Add a title
plt.title('Average Vertical Position of Lower Body over Time')

# Invert the y-axis as lower y-values represent higher positions on the screen
plt.gca().invert_yaxis()

# Display the plot
plt.show()

"""## Summary:

### Data Analysis Key Findings

*   The necessary libraries (`opencv-python` and `mediapipe`) were successfully installed, although some dependency conflicts were noted.
*   The video file "Vertical Jump.mp4" was successfully uploaded to the Colab environment.
*   The pre-trained Mediapipe Pose model was successfully loaded and initialized for video analysis.
*   The uploaded video was processed frame by frame, with the pose model detecting body landmarks.
*   Landmark data for each frame was extracted and stored.
*   An output video, 'output\_vertical\_jump.mp4', was generated with the detected landmarks overlaid.
*   A dedicated directory, 'vertical\_jump\_analysis\_results', was created to store the analysis outputs.
*   The generated output video was successfully moved into the results directory.
*   The extracted landmark data was converted into a pandas DataFrame and saved as 'landmarks\_data.csv' within the results directory.
*   A line plot visualizing the average vertical position of lower body landmarks (knees and ankles) over time was successfully generated, with the y-axis inverted to reflect spatial position correctly.

### Insights or Next Steps

*   The current analysis focuses on basic landmark tracking and visualization. Further analysis could involve calculating specific jump metrics like peak jump height, flight time, and velocity based on the landmark data.
*   The dependency conflicts reported during library installation might need to be addressed in a production environment to ensure long-term stability and compatibility with other libraries.

# Task
Perform automated vertical jump analysis from a video using a pre-trained model in Google Colab, including video integrity verification to detect tampering and counting the number of jumps. If tampering is detected, generate a report with evidence. If authentic, proceed with the analysis and save the results to a folder.

## Detection of temporal discontinuities (scene cuts)

### Subtask:
Use scene change detection algorithms to identify abrupt transitions in the video.

**Reasoning**:
Implement scene change detection by calculating frame differences and identifying frames exceeding a threshold.
"""

# Re-initialize video capture as it was released in a previous step
cap = cv2.VideoCapture(video_filename)

if not cap.isOpened():
    print(f"Error: Could not open video file {video_filename}")
else:
    # Read the first frame
    ret, prev_frame = cap.read()
    if not ret:
        print("Error: Could not read the first frame.")
    else:
        # Convert the first frame to grayscale for easier difference calculation
        prev_gray = cv2.cvtColor(prev_frame, cv2.COLOR_BGR2GRAY)

        scene_changes = []
        frame_count = 1

        while cap.isOpened():
            ret, frame = cap.read()
            if not ret:
                break

            frame_count += 1
            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)

            # Calculate the absolute difference between the current and previous frame
            frame_diff = cv2.absdiff(prev_gray, gray)

            # Calculate the mean of the difference
            mean_diff = np.mean(frame_diff)

            # Set a threshold for scene change detection. This value may need tuning.
            # A higher value means less sensitivity to small changes.
            threshold = 20.0

            if mean_diff > threshold:
                scene_changes.append(frame_count)

            prev_gray = gray

        cap.release()

        if scene_changes:
            print(f"Potential scene changes detected at frames: {scene_changes}")
        else:
            print("No significant scene changes detected.")

"""## Motion consistency analysis via optical flow

### Subtask:
Analyze the flow of pixels between frames to detect inconsistencies.

**Reasoning**:
Initialize video capture, read the first frame, convert to grayscale, and initialize parameters for optical flow, as per the instructions.
"""

# Re-initialize video capture as it was released in a previous step
cap = cv2.VideoCapture(video_filename)

if not cap.isOpened():
    print(f"Error: Could not open video file {video_filename}")
else:
    # Read the first frame
    ret, prev_frame = cap.read()
    if not ret:
        print("Error: Could not read the first frame.")
        cap.release()
    else:
        # Convert the first frame to grayscale
        prev_gray = cv2.cvtColor(prev_frame, cv2.COLOR_BGR2GRAY)

        # Parameters for lucas kanade optical flow
        lk_params = dict(winSize=(15, 15),
                         maxLevel=2,
                         criteria=(cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10, 0.03))

        # Create a list to store frames with inconsistent motion
        inconsistent_motion_frames = []

        # Find initial good features to track (optional, but helps)
        # You might need to adjust these parameters based on your video
        feature_params = dict(maxCorners=100,
                              qualityLevel=0.3,
                              minDistance=7,
                              blockSize=7)
        p0 = cv2.goodFeaturesToTrack(prev_gray, mask=None, **feature_params)

        frame_count = 1

"""**Reasoning**:
Loop through the remaining frames, calculate optical flow, check for inconsistent motion based on a threshold, and store inconsistent frame numbers.


"""

while cap.isOpened():
        ret, frame = cap.read()
        if not ret:
            break

        frame_count += 1
        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)

        # calculate optical flow
        p1, st, err = cv2.calcOpticalFlowPyrLK(prev_gray, gray, p0, None, **lk_params)

        # Select good points
        if p1 is not None:
            good_new = p1[st==1]
            good_old = p0[st==1]

            # Calculate the magnitude of the flow vectors
            flow_magnitude = np.linalg.norm(good_new - good_old, axis=1)

            # Calculate the average magnitude of the flow vectors
            if flow_magnitude.size > 0:
                average_flow_magnitude = np.mean(flow_magnitude)
            else:
                average_flow_magnitude = 0

            # Define a threshold for detecting inconsistent motion
            # This value may need tuning depending on the video and expected motion
            motion_threshold = 5.0

            if average_flow_magnitude > motion_threshold:
                inconsistent_motion_frames.append(frame_count)

            # update the previous frame and previous points
            prev_gray = gray.copy()
            p0 = good_new.reshape(-1, 1, 2)
        else:
             # If no points were tracked, consider this potentially inconsistent
             inconsistent_motion_frames.append(frame_count)
             # Re-detect features in the current frame if tracking fails completely
             p0 = cv2.goodFeaturesToTrack(gray, mask=None, **feature_params)
             prev_gray = gray.copy()


    cap.release()

    # Print the results
    if inconsistent_motion_frames:
        print(f"Potential motion inconsistencies detected at frames: {inconsistent_motion_frames}")
    else:
        print("No significant motion inconsistencies found.")

"""## Pose continuity verification using pre-trained pose estimation

### Subtask:
Check for sudden, unnatural changes in pose using the loaded pose model.

**Reasoning**:
Iterate through the results_list, calculate the average Euclidean distance between corresponding landmarks of consecutive frames, store frames where the distance exceeds a threshold, and print the results.
"""

import numpy as np

# Function to calculate Euclidean distance between two points
def euclidean_distance(p1, p2):
    return np.sqrt((p1[0] - p2[0])**2 + (p1[1] - p2[1])**2 + (p1[2] - p2[2])**2)

# Function to calculate the average distance between landmarks of two poses
def average_pose_distance(pose1_landmarks, pose2_landmarks):
    distances = []
    # Assuming pose1_landmarks and pose2_landmarks have the same number of landmarks
    for i in range(len(pose1_landmarks)):
        # Only calculate distance if both landmarks have reasonable visibility
        if pose1_landmarks[i][3] > 0.5 and pose2_landmarks[i][3] > 0.5:
             distances.append(euclidean_distance(pose1_landmarks[i][:3], pose2_landmarks[i][:3]))
    if distances:
        return np.mean(distances)
    else:
        return 0 # Return 0 if no visible landmarks to compare

# Iterate through the results_list and compare consecutive frames
pose_discontinuity_frames = []
# Define a threshold for average pose distance. This will likely need tuning.
# A higher value means less sensitivity to small pose changes.
pose_threshold = 0.05 # Example threshold, adjust as needed

if 'results_list' in locals() and len(results_list) > 1:
    for i in range(1, len(results_list)):
        frame_current = results_list[i]
        frame_previous = results_list[i-1]

        current_landmarks = frame_current['landmarks']
        previous_landmarks = frame_previous['landmarks']

        # Ensure both frames have landmark data before comparison
        if current_landmarks and previous_landmarks:
            avg_dist = average_pose_distance(current_landmarks, previous_landmarks)

            if avg_dist > pose_threshold:
                pose_discontinuity_frames.append(frame_current['frame'])
        else:
            # If landmarks are missing in a frame, it could indicate a discontinuity
            if not current_landmarks and i+1 <= len(results_list):
                 pose_discontinuity_frames.append(frame_current['frame'])


    # Print the list of frames with potential pose discontinuities
    if pose_discontinuity_frames:
        print(f"Potential pose discontinuities detected at frames: {pose_discontinuity_frames}")
    else:
        print("No significant pose discontinuities detected.")
else:
    print("results_list is not available or does not have enough data for pose discontinuity check.")

"""## Metadata and frame rate consistency checks

### Subtask:
Verify if frame rates and other metadata are consistent throughout the video.

**Reasoning**:
Re-initialize the video capture object, get the total number of frames and fps, and print them.
"""

import cv2

# 1. Re-initialize the video capture object
cap = cv2.VideoCapture(video_filename)

# 2. Check if the video capture object was opened successfully
if not cap.isOpened():
    print(f"Error: Could not open video file {video_filename}")
else:
    # 3. Get the total number of frames
    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    # 4. Get the frames per second (fps)
    retrieved_fps = int(cap.get(cv2.CAP_PROP_FPS))

    # 5. Print the total number of frames and the frames per second
    print(f"Total number of frames: {total_frames}")
    print(f"Frames per second (retrieved): {retrieved_fps}")

    # 6. Release the video capture object
    cap.release()

"""**Reasoning**:
Compare the retrieved fps with the previously obtained fps and mention the limitation of basic OpenCV for comprehensive metadata checks.


"""

# 7. Compare the retrieved fps with the fps obtained during initial video processing
# Assuming 'fps' variable from previous processing step is available in the kernel state
# Note: This comparison is a simple check and not a guarantee of full metadata consistency.
if 'fps' in locals() and abs(retrieved_fps - fps) > 1: # Allow a small tolerance
    print(f"Warning: Retrieved FPS ({retrieved_fps}) is significantly different from previously obtained FPS ({fps}).")
else:
    print("Retrieved FPS is consistent with the previously obtained FPS.")

# 8. Briefly mention that comprehensive metadata checks require external tools
print("\nNote: Basic OpenCV provides limited metadata verification.")
print("Comprehensive video metadata and consistency checks often require external tools or libraries.")

"""## Outcome decision and report generation

### Subtask:
Based on the integrity checks, decide if the video is authentic. Generate a detailed report if tampering is suspected, including evidence.

**Reasoning**:
Define a threshold, count inconsistencies, compare with the threshold, and generate a report summarizing all integrity checks, saving it to a file, then finish the task.
"""

# 1. Define a threshold for the number of detected inconsistencies
# This value will need tuning based on what level of inconsistency is acceptable.
inconsistency_threshold = 30  # Example threshold

# Ensure the lists exist, initialize if not (handles cases where previous steps might have failed)
if 'scene_changes' not in locals():
    scene_changes = []
if 'inconsistent_motion_frames' not in locals():
    inconsistent_motion_frames = []
if 'pose_discontinuity_frames' not in locals():
    pose_discontinuity_frames = []
if 'results_dir' not in locals():
    results_dir = '.' # Use current directory if results_dir wasn't created


# 2. Count the total number of detected inconsistencies
total_inconsistencies = len(scene_changes) + len(inconsistent_motion_frames) + len(pose_discontinuity_frames)

# 3. Compare the total number of inconsistencies with the defined threshold
# 4. If the total number of inconsistencies exceeds the threshold, conclude tampering is suspected
if total_inconsistencies > inconsistency_threshold:
    tampering_suspected = True
    print(f"Tampering suspected: Total inconsistencies ({total_inconsistencies}) exceed threshold ({inconsistency_threshold}).")
else:
    tampering_suspected = False
    print(f"Video appears authentic: Total inconsistencies ({total_inconsistencies}) do not exceed threshold ({inconsistency_threshold}).")

# 6. Regardless of the outcome, generate a report summarizing the findings
report_content = "Vertical Jump Video Integrity Report\n"
report_content += "=====================================\n\n"

if tampering_suspected:
    report_content += "Conclusion: TAMPERING IS SUSPECTED.\n"
    report_content += f"Total inconsistencies detected: {total_inconsistencies}\n"
    report_content += f"Inconsistency threshold: {inconsistency_threshold}\n\n"
    report_content += "Evidence of potential tampering:\n"
    if scene_changes:
        report_content += f"- Scene changes detected at frames: {scene_changes}\n"
    if inconsistent_motion_frames:
        report_content += f"- Motion inconsistencies detected at frames: {inconsistent_motion_frames}\n"
    if pose_discontinuity_frames:
        report_content += f"- Pose discontinuities detected at frames: {pose_discontinuity_frames}\n"
else:
    report_content += "Conclusion: Video appears AUTHENTIC.\n"
    report_content += f"Total inconsistencies detected: {total_inconsistencies}\n"
    report_content += f"Inconsistency threshold: {inconsistency_threshold}\n\n"
    report_content += "Details of integrity checks:\n"
    report_content += f"- Scene changes detected at frames: {scene_changes}\n"
    report_content += f"- Motion inconsistencies detected at frames: {inconsistent_motion_frames}\n"
    report_content += f"- Pose discontinuities detected at frames: {pose_discontinuity_frames}\n"

# Include metadata check results if available
if 'retrieved_fps' in locals() and 'fps' in locals():
     report_content += f"\nMetadata Check:\n"
     report_content += f"- Retrieved FPS: {retrieved_fps}\n"
     report_content += f"- Previously obtained FPS: {fps}\n"
     # Assuming the consistency check message is indicative enough, or add explicit check result
     if abs(retrieved_fps - fps) > 1:
         report_content += "- FPS consistency: Inconsistent\n"
     else:
          report_content += "- FPS consistency: Consistent\n"


# Save the report to a text file
report_filename = os.path.join(results_dir, 'integrity_report.txt') # Use os.path.join for path
try:
    with open(report_filename, 'w') as f:
        f.write(report_content)
    print(f"\nIntegrity report saved to '{report_filename}'")
except IOError as e:
    print(f"Error saving integrity report to '{report_filename}': {e}")

"""## Summary:

### Data Analysis Key Findings

*   No significant scene changes were detected in the video based on the frame difference analysis with a threshold of 20.0.
*   Potential motion inconsistencies were detected in several frames based on the average optical flow magnitude exceeding a threshold of 5.0. Frames flagged include [44, 45, 46, 56, 63, 109, 110, 143, 144, 166, 167, 170, 184, 206].
*   Potential pose discontinuities were detected in numerous frames based on the average Euclidean distance between consecutive frame landmarks exceeding a threshold of 0.05.
*   The video has a total of 343 frames and a retrieved frame rate of 28 FPS, which was consistent with the previously obtained FPS.
*   A total of 57 inconsistencies (sum of scene changes, motion inconsistencies, and pose discontinuities) were detected across all checks.
*   Based on a defined inconsistency threshold of 30, tampering was suspected as the total inconsistencies (57) exceeded this threshold.
*   An integrity report summarizing all findings and the conclusion was generated and saved as `integrity_report.txt`.

### Insights or Next Steps

*   The thresholds used for detecting scene changes, motion inconsistencies, and pose discontinuities significantly impact the outcome. Further tuning of these thresholds based on the specific characteristics of vertical jump videos may be necessary for more accurate tamper detection.
*   While tampering is suspected, the analysis did not definitively prove tampering. The detected inconsistencies could potentially be due to natural variations in movement, camera shake, or limitations of the detection algorithms. A manual review of the flagged frames and further investigation using more robust or diverse integrity checks could provide a more conclusive result.

"""

!ls -lh

from tensorflow.keras.applications import MobileNetV2

# Load a pre-trained model
model = MobileNetV2(weights='imagenet')

# Save the model as .h5
model.save('model.h5')

from google.colab import files
files.download('model.h5')